\documentclass{article}

%
% Hyphenation etc...
%
\usepackage{polyglossia}
\setdefaultlanguage[variant=british]{english}
\usepackage{csquotes}


%
% General document formatting
%
\usepackage{url}
\usepackage{xcolor}
% \usepackage[margin=0.7in]{geometry}
% \usepackage[parfill]{parskip}
% \usepackage[utf8]{inputenc}  % Not needed under xelatex


%
% Related to math
%
\usepackage{amsmath,amssymb,amsfonts,amsthm}


%
% Change tracking
%
\usepackage{changes}
\definechangesauthor[name={John Reid}, color=orange]{JR}
\newcommand{\jr}[1]{\added[id=JR]{[#1]}}


%
% Bibliography
%
\usepackage[
    backend=biber,
    % doi=false,
    isbn=false,
    url=false,
    % style=apa,
    % style=humanmutation,
    sorting=none,
    natbib=true,
  ]{biblatex}
\bibliography{CAGI5-reg-sat} % or
% \addbibresource{<database>.<extension>}


%
% Command definitions
%
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}


%
% Authorship and title
%
\author{
  Alex Hawkins-Hooker \\
  \texttt{first2.last2@xxxxx.com}
  \and
  Henry Kenlay \\
  \texttt{first2.last2@xxxxx.com}
  \and
  John E. Reid \\
  \texttt{johnbaronreid@gmail.com}
}
\title{A method for the CAGI 5 regulation saturation challenge}


\begin{document}

\maketitle


\todo{Any suggestions for catchy names for our method (acronyms or otherwise)?}


\section*{Introduction}

\subsection*{Biological context}


\subsection*{Existing/similar methods}


\subsection*{Data}

17,500 SNVs in five enhancers and nine promoters (collectively \emph{regulatory
elements}) were assayed in a saturation mutagenesis massively parallel reporter
assay (MPRA). A linear regression model was fit to the data. This resulted in a
\texttt{Confidence} score (a function of the associated $p$-value) and
\texttt{Value} score for each SNV. Briefly, the \texttt{Confidence} represents
how sure we are that the SNV has an effect on gene expression and the
\texttt{Value} represents the direction and magnitude (positive for activating,
negative for repressing) of the effect (see Figures~\ref{fig:conf-value}
and~\ref{fig:training-sort1}). The SNVs were classified into three classes: if
the \texttt{Confidence} is less than .1, the \texttt{Direction} is 0; otherwise
the \texttt{Direction} is 1 (resp. -1) if \texttt{Value} $\ge 0$ (resp. $< 0$).

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig-conf-value-scatter}
\caption{Scatter plot of the confidence and value scores in the training data coloured
by regulatory element and cell line.}
\label{fig:conf-value}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=\textwidth]{fig-value-conf-SORT1}
\caption{Training data for the SORT1 regulatory element.}
\label{fig:training-sort1}
\end{figure}

One regulatory element, \texttt{TERT}, was assayed in two cell lines. We
distinguish these experiments by \texttt{TERT-GBM} and \texttt{TERT-HEK293T}.


\subsection*{Prediction task}

The data was split 25\% - 75\% into training and test data. Note that the
training data was chosen to form contiguous chunks (see
Figure~\ref{fig:training-sort1}).  The challenge required the participating
teams to enter a prediction for each SNV in the test data. Each prediction
consisted of:
\begin{itemize}
  \item A categorical prediction of the \texttt{Direction} of the effect (-1, 0 or 1).
  \item The probability of correctly assigning the predicted \texttt{Direction},
    that is a measure of confidence in the prediction.
  \item A prediction of the \texttt{Confidence}.
  \item The standard error of the predicted \texttt{Confidence}.
\end{itemize}
The assessment statistics were loosely hinted at in the challenge description
but were far from well-defined.


\section*{Methods}

We were inspired by Zeng et al.'s winning
method~\cite{ZengAccurateeQTLprioritization2017} from the CAGI~4
challenge~\citep{KreimerPredictinggeneexpression2017}.


\subsection*{Features}

We tested different types of feature to assess their predictive power.


\subsubsection*{Conservation}

We used \emph{phyloP}~\cite{PollardDetectionnonneutralsubstitution2010},
\emph{phastCons}~\cite{SiepelEvolutionarilyconservedelements2005}, \emph{GerpN}
and \emph{GerpRS}~\cite{CooperDistributionintensityconstraint2005a} base-pair
resolved conservation scores.

Downloaded from UCSC~\cite{KuhnUCSCgenomebrowser2013}? \jr{AHH: can you check
where this came from?}


\subsubsection*{DNase hypersensitivity}

For each regulatory element we identified the closest matching
ENCODE~\cite{DunhamintegratedencyclopediaDNA2012} cell type for which there is
a DNase-hypersensitivity track (see Table~\ref{tab:encode-cell-types}). We
downloaded the tracks~\cite{RosenbloomENCODEDataUCSC2012} and created
41-dimensional features consisting of the signal at each base-pair flanked by
the signal at the bases 20bp on either side.

\begin{table}[htp]
\resizebox{\textwidth}{!} {
\begin{tabular}{rll}
  \\
  challenge  & ENCODE    & UCSC track \\
  \hline
  HepG2      & HepG2     & wgEncodeOpenChromDnaseHepg2BaseOverlapSignal \\
  HEL 92.1.7 & GM12878   & wgEncodeOpenChromDnaseGm12878BaseOverlapSignal \\
  HEK293T    & HEK293T   & wgEncodeOpenChromDnaseHek293tBaseOverlapSignal \\
  K562       & K562      & wgEncodeOpenChromDnaseK562BaseOverlapSignalV2 \\
  GBM        & Gliobla   & wgEncodeOpenChromDnaseGlioblaBaseOverlapSignal \\
  SK-MEL-28  & Colo829   & wgEncodeOpenChromDnaseColo829BaseOverlapSignal \\
  HaCaT      & NHEK      & wgEncodeOpenChromDnaseNhekBaseOverlapSignal \\
  MIN6       & PanIslets & wgEncodeOpenChromDnasePanisletsBaseOverlapSignal
\end{tabular}
}
\caption{The ENCODE cell lines that were identified as the closest
matches to the cell lines in the challenge. The corresponding UCSC track
names are also given.}
\label{tab:encode-cell-types}
\end{table}


\subsubsection*{DeepSea neural networks}

We trained several different neural network architectures on the genomic
prediction benchmark detailed in the original DeepSea
paper~\cite{ZhouPredictingeffectsnoncoding2015}. We evaluated these networks on
a region surrounding each variant twice, once with the reference allele and
once with the alternate allele. Features were generated as the difference in
activations between the two evaluations of internal and output layers of the
networks.

DanQ network~\cite{QuangDanQhybridconvolutional2016}.

\jr{AHH: We need some more details here. If I let you know which network features I used,
can you fill in the details of the network architectures and training?}


\subsubsection*{Region identity}

We one-hot encoded the identity of region to use as a feature.


\subsubsection*{Substitution}

We one-hot encoded the reference to alternate allele substitution as a feature.


\subsubsection*{Stacked predictions}

We generated features estimating the \texttt{Value} and \texttt{Confidence} by
fitting models to held out data. For the test SNVs, the held out data was the
training data. During cross-validation, features for the validation SNVs' were
estimated using the training folds as the held out data. We used the DeepSea
difference features, DNase features and conservation features to fit the
gradient boosting models estimating these generated features.

\begin{figure}
\includegraphics[width=\textwidth]{fig-stacking-x-validation}
\caption{Predictions used for stacking coloured by regulatory element.
  \emph{Top left}: Overall and regulatory element specific AUROC for prediction
  of \texttt{Direction} using \texttt{Value}.
  \emph{Top right}: Overall and regulatory element specific AUPRC for prediction
  of \texttt{Direction} using \texttt{Value}.
  \emph{Bottom left}: Predicted \texttt{Value} against actual \texttt{Value}.
  \emph{Bottom right}: Predicted \texttt{Confidence} against actual \texttt{Confidence}.
}
\label{fig:stacking}
\end{figure}


\subsection*{Inference}


\subsubsection*{Models}

We used three different gradient boosting algorithms:
\emph{XGBoost}~\cite{ChenXGBoostScalableTree2016},
\emph{CatBoost}~\cite{ProkhorenkovaCatBoostunbiasedboosting2017} and
\emph{LightGBM}~\cite{KeLightGBMHighlyEfficient2017}. We used the gradient
boosting algorithms to regress the \texttt{Value} and \texttt{Confidence}, we
also used them as classifers to predict the \texttt{Direction}. We tested
different subsets of features in a 5-fold cross-validation set up to identify
the best features.  Models were assessed by the cross-validated one against
many area-under-precision-recall-curve (AUPRC). We stacked models in that we
used the prediction of cross-validated regression models of the \texttt{Value}
and \texttt{Confidence} as features in the classification model for the
\texttt{Direction}.


\subsubsection*{Cross-validation}

We used a custom cross-validation configuration to investigate different models
and hyper-parameters.  The spatial dependence of the \texttt{Value} in each
regulatory element would clearly break the independence assumption in a naive
5-fold cross-validation.  Therefore we choose to perform 5-fold
cross-validation that retained spatial dependencies. Briefly, we defined chunks
as maximal contiguous regions in the training data for each regulatory element.

As the validation metrics for the challenge were ill-defined and the challenge
requires several predictions for each SNV across 3 categorical outcomes, we
chose to use a simplified task to evaluate models and hyperparameters. We used
the absolute predicted \texttt{Value} as a predictor of the absolute
\texttt{Direction}.  This is a binary classification task and permits analysis
by receiver operating characteristic (ROC) and precision-recall (PR) curves. We
used the area under the precision-recall curve (AUPRC) as our primary
evaluation criteria when performing cross-validation. We also calculated the
area under the ROC curve (AUROC) as a statistic of interest.

\todo{Precisely describe cross-validation scheme respecting structure of
training loci.}



\subsubsection*{Hyper-parameter Bayesian optimisation}

\todo{Describe Bayesian optimisation scheme}


\subsubsection*{Confidence standard error}

To estimate the standard error of the \texttt{Confidence} we trained an
ensemble of regression models and used the standard deviation of the
predictions to estimate the standard error.


\subsection*{Variable importance}

\todo{Discuss which features were important for prediction.}


\section*{Contributions}

HK, AHH and JR developed the deep convolutional neural network models of
genomic sequence. AHH and JR derived the features required for this
project. AHH and JR fit the gradient boosting machines. JR wrote the
manuscript.

All code to reproduce the submitted results is available in a git
repository~\url{git@bitbucket.org:alex-hh/cagimpra.git} which can be made
available on request.


\subsection*{Acknowledgements}

We would like to thank Elena Vigorito, Lorenz Wernisch, Paul Newcombe, and Paul
Kirk for insightful comments on parts of this work.

%
% Bibliography
%
\printbibliography

\end{document}
